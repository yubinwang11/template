---
title: Brief Report of Learning-Based Observer
date: 2021-9-12
permalink: /posts/2021/09/learn-obs
excerpt_separator: <!--more-->
toc: true
header:
 og_image: "posts/geom-sf-facet/shared_legend_right-1.png"
tags:
  - machine learning
  - control theory
  
---
This report serves as my group meeting materials at EMANG in August,27. This article is about using the end-to-end approach of Deep Learning to compute the transformation which is incredibly essential for KKL observer. For simple, this article dives into this field through the aspect of autonomous systems and then the work is extended to non-autonomous systems with an excitation.

<!--more-->

# Problem Statement

nonlinear discrete-time systems of general form:      

$$
\left\{\begin{array}{l}
x_{k+1}=F\left(x_{k}, u_{k}\right) \\
y_{k}=h\left(x_{k}\right)
\end{array}\right.
\tag{1}
$$

## Assumption

### Assumption 1

$F$ is invertible and $F^{-1}$ and $h$ are of class $C^1$ and globally Lipschitz. For all non-negative integers $i$ we denote $◦$  the composition operator and    

$$F^{i}=\underbrace{F \circ F \circ \cdots \circ F}_{i \text { times }}, \quad F^{-i}=\left(F^{-1}\right)^{i} \tag{2}$$

### Assumption 2

For all $(x_1, x_2) ∈ χ^2$ of nonlinear systems with input $u$, if $x_1 ≠x_2$, then there exists a positive integer $i$ such that $h(F^{−i} (x_1)) ≠ h(F^{−i} (x_2))$. This means that for any input $u$ of interest, there exists an open bounded set $\mathcal{O}$ containing χ such that nonlinear system is backward.



# Preliminaries of Relevant Mapping and Observer

For autonomous systems:       

$$
\left\{\begin{array}{l}
x_{k+1}=F\left(x_{k}, 0\right) \\
y_{k}=h\left(x_{k}\right)
\end{array}\right. \tag{3}
$$      

with the Luenberger-like methodology holding the two traditional assumption.

For almost any controllable pair $(A, B)$ of dimension $d_z:= d_y (d_x + 1)$ with $A$ Hurwitz there exists a map $T: \mathcal{X} \rightarrow \mathbb{R}^{d_{z}}$ that satisfies:     

$$
T(F(x))=A T(x)+B h(x) \quad \forall x \in \mathcal{X}
\tag{4}
$$
a pseudo-inverse $T ^{−1}$ such that the following system is an observer for autonomous system     

$$
\begin{aligned}
z_{k+1} &=A z_{k}+B y_{k} \\
\hat{x}_{k} &=T^{-1}\left(z_{k}\right)
\end{aligned}
\tag{5}   
$$      

The unique solution of the map is given by    

$$
T(x)=\sum_{i=0}^{+\infty} A^{i} B h\left(F^{-(i+1)}(x)\right)
\tag{6}
$$

## Theorem

Assume that assumptions hold for autonomous system and let $d_z = d_y (d_x + 1)$. Then, for any matrix $A = Diag (λ_1, . . . , λ_{d_z }) ∈ \mathbb{R}^ {d_z×d_z} $ with eigenvalues $ (λ1, . . . , λ^{d_z} )$ and $ ρ(A) < 1 $ and $B$ the vector $(1, \ldots, 1)^{\top}$in $\mathbb{R}^{d_x}$ .

There exists an injective mapping $T : X → \mathbb{R} ^{dz}$ and a left inverse  $T^{−1}$ such that the trajectories remaining in $\mathcal{X}$ and the trajectories of observer dynamics satisfy     

$$
\begin{array}{l}
\lim _{k \rightarrow+\infty}\left|T\left(X_{k}\left(x_{0}\right)\right)-Z_{k}\left(x_{0}, z_{0}\right)\right|=0 \\
\lim _{k \rightarrow+\infty}\left|X_{k}\left(x_{0}\right)-T^{-1}\left(Z_{k}\left(x_{0}, z_{0}\right)\right)\right|=0
\end{array}   \tag{7}
$$

## KKL Observer

design a KKL observer to online estimate $x_k$ from the knowledge of a sequence of the past and current values of the output $y_k$ and input $u_k$.



## Traditional Ansatz

An injective and continuous map $T$ which transforms the target system into a Hurwitz form exists.  Then, thanks to the contraction properties, the implementation of this new dynamic with any initial condition then provides an asymptotically convergent estimate of $T(X(x_0, u))$. Since $T$ is injective, an estimate of the solution $X(x_0, u)$can be obtained. 



## Objective

However, even if sufficient conditions for the existence of this map have been given for a general form of nonlinear continuous systems, and consequently of the observer, a computable solution is unfortunately often difficult or impossible to obtain.

Unfortunately, in most situations the function $T$ given by the unique solution is difficult to compute and an explicit expression for its pseudo inverse is rarely available. Therefore,  the object is  to develop an algorithm to numerically identify these mappings.



## Why choose the learning-based techniques

Deep neural network architectures have shown to be able to learn non-linear mappings and to generalize to unseen data under certain conditions for a diverse range of problems. Different variants of the universal approximation theorem guarantee that a multi layer perceptron can express any arbitrary function, including our desired mappings, under mild conditions either for infinitely wide or infinitely deep (i.e layered)  model architectures.



# Deep Learning-Based Observer Design

This section proposes a constructive method to learn the mappings $T$ and $T^{−1}$ from data using unsupervised  learning.

## Model Structure

The network architecture of proposed model is shown.


![](C:\Users\fishc\Desktop\EMAN Group\Internship\Learning-Based Observer\Brief Report of _Deep Learning-based Luenberger observers design for discrete-time nonlinear systems__md_files\Model Structure.png)

### Mapping Process

Identify a latent space $z = T(x)$, which respects the observer dynamics given by the mapping.
enforced dynamic using the following loss:    

$$
\mathcal{L}_{d y n}=\left\|T\left(x_{k+1}\right)-\left(A T\left(x_{k}\right)+B h\left(x_{k}\right)\right)\right\|
\tag{8}
$$

### Recovery Process

Seek to learn the pseudo-inverse $T ^{−1}$ so that an estimation $\hat{x}$ of the state is be recovered, which is achieved using an reconstruction loss of the auto-encoder, where $T$ is the encoder and $T ^{−1}$ is the decoder:       

$$
\mathcal{L}_{\text {recon }}=\left\|x_{k}-T^{-1}\left(T\left(x_{k}\right)\right)\right\| 
\tag{9}
$$

## Training

Train the model by minimizing two loss functions on a large dataset $D=\left\{x_{k}, x_{k+1}\right\}$, which is generated automatically from the considered systems. To allow a proper exploration of the state space, samples $x_k$ are obtained from a uniform random distribution on $\mathcal{X}$ while samples $x_{k+1}$ are obtained by applying the model dynamic.

A difficulty arising with the chosen learning scheme is that the values of the latent representation $z = T(x)$ in losses are not part of the dataset $D$. It is therefore not possible to scale them, which is an important step in machine learning ensuring that neural networks operate in a favorable regime of their point-wise non-linearities.

### Extended Mapping Capability

To cope with the below proble, $z$ can be scaled during the learning phase by adapting $B$. Taking  $\tilde{B}=\operatorname{diag}\left(\left[b_{1}, b_{2}, \ldots\right]\right) B$ and the scaled $T$ can be described as          

$$
\begin{aligned}
\tilde{T}(x) &=\sum_{i=0}^{+\infty} A^{i} \operatorname{diag}\left(\left[b_{1}, b_{2}, \ldots\right]\right) B h\left(f^{-(i+1)}(x)\right) \\
&=\operatorname{diag}\left(\left[b_{1}, b_{2}, \ldots\right]\right) T(x)
\end{aligned}
\tag{10}
$$

### Algorithm

Pseudo code of this kind of algorithm is shown as following. 

Notice that $B$ is iteratively adapted to standardize $z$ with respect to its standard deviation.
![](C:\Users\fishc\Desktop\EMAN Group\Internship\Learning-Based Observer\Brief Report of _Deep Learning-based Luenberger observers design for discrete-time nonlinear systems__md_files\Algorithm.png)

## Non-Autonomous Systems

To generalize this estimation method, this section consider generalizing this implementation to non-autonomous systems with strict conditions of control input.

Assume target system is standard nonlinear system with input,  and $u^0$ a constant input. In this case, two assumptions hold. From Theorem mentioned there exists a map $T$ solution to      

$$
T\left(F\left(x_{k}, u^{0}\right)\right)=A T\left(x_{k}\right)+B y_{k}
\tag{11}
$$

when adding mapping $T\left(F\left(x_{k}, u_{k}\right)\right)$ to each side, the equation becomes      

$$
T\left(F\left(x_{k}, u_{k}\right)\right)=A T\left(x_{k}\right)+B y_{k}+T\left(F\left(x_{k}, u_{k}\right)\right)-T\left(F\left(x_{k}, u^{0}\right)\right)
\tag{12}
$$
Hence, the latent state $$
z_{k}=T\left(x_{k}\right)
$$ evolves according to observer dynamics     

$$
z_{k+1}=A z_{k}+B y_{k}+\Psi\left(z_{k}, u_{k}\right)
\tag{13}
$$
where  

$$
\Psi\left(z_{k}, u_{k}\right)=T\left(F\left(T^{-1}\left(z_{k}\right), u_{k}\right)\right)-T\left(F\left(T^{-1}\left(z_{k}\right), u^{0}\right)\right)
\tag{14}
$$

Note that, in the autonomous case, the term $Ψ(z_k, u_k)$ does not appear, the dynamics of z were contracting and it was enough to asymptotically simulate z with any initial condition.

### Note

Unfortunately this is no longer true for non-autonomous systems, and this condition would be hard to cope with.

But the following method gives us a way.

Let $u_0$ be a constant input such that assumptions hold. Assume that $A$ can be designed such that $T, T _{−1}$ given by Theorem satisfy  

$$
\left|\Psi\left(z^{1}, u\right)-\Psi\left(z^{2}, u\right)\right| \leq C_{u}\left|z^{1}-z^{2}\right|
\tag{15}
$$

for all $u \in \mathcal{U}$ and for all $z_1 , z_2$. Then for $u$ such that $ρ(A + CuI) < 1$ , we get that any solution verifies  

$$
\lim _{k \rightarrow+\infty}\left|x_{k}-T^{-1}\left(z_{k}\right)\right|=0
\tag{16}
$$

### Proof

To take a step further, let denote by $e_k$ the estimation error $e_{k+1} = z_{k+1} − T(x_{k+1})$. $T$ being injective it is sufficient to prove that $e_{k+1} = z_{k+1} − T(x_{k+1})$ converges geometrically towards zero.  

$$
\begin{aligned}
e_{k+1} &=z_{k+1}-T\left(F\left(x_{k}, u_{k}\right)\right.\\
&=A\left(z_{k}-T\left(x_{k}\right)\right)+\Psi\left(z_{k}, u_{k}\right)-\Psi\left(T\left(x_{k}\right), u_{k}\right)
\end{aligned}
\tag{17}
$$
Assuming that            

$Ψ(z_k, u_k)$ is Lipschitz 

$u_k − u_0$ small enough  

$ρ(A)$ small enough,

dynamics of non-autonomous systems are still  contracting and $z_{k+1} − T(x_{k+1})$ converges towards zero.



Hence the mappings $T$ and $T^{−1}$ , learned on the autonomous system following the procedure of autonomous systems, can be used to design the observer of non-autonomous systems.

### Miscs 

Another paper about numerical design of luenberger observers proposes to select an input $u(t)$ that makes the system extensively explore the state space. Then a time varying mapping $T_u$ (and so a time varying pseudo inverse $T ^{−1}_ u $) is identified. Unfortunately, this procedure does not guarantee a proper exploration of the state space during training, especially for systems which quickly converge to a limit cycle.

And the author did rely on the stability of the observer, and therefore on the fact that $z$ forgets its initial conditions, to eliminate the early stages of the simulation from the dataset. Moreover,  the mapping $z = T(x)$ is learned with a fully supervised objective.

Compared with the mentioned paper, this paper avoids these limitations by design through unsupervised learning, and is therefore simpler to apply. 

First, there are not constraints on inputs, which can be selected to properly explore that state space. Furthermore, the task of learning $T$ may be easier as it is not time dependent. More importantly, in the case of a time varying mapping, an identification should be available for the (possibly infinite) duration of the observer experiment.

But remember, even if the performances of the proposed approach are promising when we adopt constant control input, the design assumptions in non-autonomous case may be relaxed to provide more general results. And the performance when cope with time-varying control input seems unclear since the computation cost would be unaffordable.



# Simulation Reproduction 

Consider  non-conservative Van der Pol oscillator with nonlinear damping 
             
$$
\left\{\begin{array}{l}
x_{1, k+1}=x_{1, k}+\delta_{t} x_{2, k} \\
x_{2, k+1}=x_{2, k}+\delta_{t}\left(\left(1-x_{1, k}^{2}\right) x_{2, k}-x_{1, k}+u_{k}\right) \\
y_{k}=x_{1, k}
\end{array}\right.
\tag{18}
$$
the observer dynamics  

$$
z_{k+1}=A z_{k}+B y_{k}+\Psi\left(z_{k}, u_{k}\right) ,  z_{k}=T\left(x_{k}\right)
\tag{19}
$$ 

Select $A=diag(0.5,0,-0.5)$,

makes use of $T$ and $T ^{−1}$ learned on the unforced system when $u=0$ for convenience

During evaluation, consider use an input signal $u_k = 0.44 cos(0.5k × δ_t)$

## Check Contraction Property

1. $u_k = 0.44 cos(0.5k × δ_t)≤0.44$ and can be deemed as measurement noise. (small enough)  corrected
2. spectral radius $\rho(A)= 0.5<1$ (small enough) corrected

## Data

Training data and validation data are uniformly sampled in $\mathcal{X}=[-2.3,2.3] \times [-3.3,3.3]$.

To achieve more accurate estimation, training data and validation data can be generated from different range. For example, training data is from $\mathcal{X}=[-2.5,2.5] \times[-3.5,3.5]$, while validation experiments are conducted with $x_0∈\mathcal{X_0}=[-2,2] \times[-3,3]$.

## Noise on y

Additional noise is considered on measurements (a Gaussian signal with standard deviation of 0.2 is added to $y_k$).

## Results

$T$  30 epochs, $T^{-1}$30 epochs

![](C:\Users\fishc\Desktop\EMAN Group\Internship\Learning-Based Observer\Brief Report of _Deep Learning-based Luenberger observers design for discrete-time nonlinear systems__md_files\Results\T30 T-1 30\T30 T-1 30.jpg)

$T$  80 epochs, $T^{-1}$80 epochs

![](C:\Users\fishc\Desktop\EMAN Group\Internship\Learning-Based Observer\Brief Report of _Deep Learning-based Luenberger observers design for discrete-time nonlinear systems__md_files\Results\T 80 T-1 80\T 80 T-1 80.jpg)

$T$  160 epochs, $T^{-1}$160 epochs

![](C:\Users\fishc\Desktop\EMAN Group\Internship\Learning-Based Observer\Brief Report of _Deep Learning-based Luenberger observers design for discrete-time nonlinear systems__md_files\Results\T160 T-1 160\T 160 T-1 160.jpg)

# Discussion

## The choice of $A$

Note:  the choice of $A$ spectrum can actually effects the capacity of noise rejection, to be more specific, the spectra radius of $A$ is larger, the ability to reject the measurement noise on y is more robust.

However, according to the analysis of non-autonomous systems' contracting property, whether the learning-based observer performance with an excitation can converge or not is dependent on the spectra radius of $A$ which means the smaller $A$ is better.  

So It would be a trade-off between the convergence speed and the capability of noise rejection.

### Results

$A=diag(0.9,0,-0.9)$,  $T$  80 epochs, $T^{-1}$80 epochs

![](C:\Users\fishc\Desktop\EMAN Group\Internship\Learning-Based Observer\Brief Report of _Deep Learning-based Luenberger observers design for discrete-time nonlinear systems__md_files\Results\T T-1 80 noise 0.2 A 0.9\errors.jpg)

$A=diag(0.5,0,-0.5)$,  $T$  80 epochs, $T^{-1}$80 epochs

![](C:\Users\fishc\Desktop\EMAN Group\Internship\Learning-Based Observer\Brief Report of _Deep Learning-based Luenberger observers design for discrete-time nonlinear systems__md_files\Results\T 80 T-1 80\T 80 T-1 80.jpg)

$A=diag(0.2,0,-0.2)$,  $T$  80 epochs, $T^{-1}$80 epochs

![](C:\Users\fishc\Desktop\EMAN Group\Internship\Learning-Based Observer\Brief Report of _Deep Learning-based Luenberger observers design for discrete-time nonlinear systems__md_files\Results\T T-1 80 R(A)=0.2 noise=0.2\errors (1).jpg)



### Model Uncertainty

To generalize the robustness of this learning-based KKL observer,  we have already considered the measurement noise on output y. And performance verifies the capability of noise rejection.

To take a step further, let's take a look at the nonlinear discreet time system with model uncertainty, like below. 

$$
\left\{\begin{array}{l}
x_{1, k+1}=x_{1, k}+\delta_{t} x_{2, k}+\sigma_1 \\
x_{2, k+1}=x_{2, k}+\delta_{t}\left(\left(1-x_{1, k}^{2}\right) x_{2, k}-x_{1, k}+u_{k}\right)+\sigma_2 \\
y_{k}=x_{1, k}
\end{array}\right.
\tag{20}
$$
$\sigma_1,\sigma_2$ both subject to $(0,0.1)$

In this case, the modification aims to learn an appropriate mapping via inaccurate dataset. Here are the results.

 ### Results

$T$  130 epochs, $T^{-1}$130 epochs, both measurement noise on y and model uncertainty on x exist

![](C:\Users\fishc\Desktop\EMAN Group\Internship\Learning-Based Observer\Brief Report of _Deep Learning-based Luenberger observers design for discrete-time nonlinear systems__md_files\Results\T 130 T-1 130 uncertainty_std=0.1 nosie_std=0.2\130 0.2 0.1.jpg)

Moreover, to test the disturbance of model uncertainty to performance, the measurement noise on y was removed. 

$T$  130 epochs, $T^{-1}$130 epochs, solely model uncertainty on $x$ exists

![](C:\Users\fishc\Desktop\EMAN Group\Internship\Learning-Based Observer\Brief Report of _Deep Learning-based Luenberger observers design for discrete-time nonlinear systems__md_files\Results\T T-1 130 unc_std=0.1 noise=0\T T-1 130 unc_std=0.1 noise=0.jpg)

## Future Work

### hyper-parameter 

### EKF QR Matrix

### Trade-off of A

### Robustness of model uncertainty



# Appendix

the relevant proof about non-autonomous systems' contracting property

![](C:\Users\fishc\Desktop\EMAN Group\Internship\Learning-Based Observer\Brief Report of _Deep Learning-based Luenberger observers design for discrete-time nonlinear systems__md_files\proof1.jpg)

![](C:\Users\fishc\Desktop\EMAN Group\Internship\Learning-Based Observer\Brief Report of _Deep Learning-based Luenberger observers design for discrete-time nonlinear systems__md_files\proof2.jpg)

